1. 常见的激活函数选择：
ReLU（Rectified Linear Unit）： ReLU 是一个常用的激活函数，通常用于隐藏层。这个函数的输出是输入的正值（如果输入是负值，那么输出为0）。ReLU 由于其计算效率和有效性（比如帮助缓解梯度消失问题）在深度学习中广泛使用。
Leaky ReLU： Leaky ReLU 是 ReLU 的一个变体，它允许负输入有一个小的非零输出，这样可以防止神经元完全死亡，即永久性不活动。
Tanh： Tanh（双曲正切）函数将输入值压缩到 -1 和 1 之间，这在一些情况下是很有用的，例如当输出需要有一定范围时。
Sigmoid： Sigmoid 函数将输入值压缩到 0 和 1 之间。这在二元分类问题中很有用，例如在判别器的输出层，我们可能会使用 sigmoid 函数来表示生成的数据是真实的（接近 1）或假的（接近 0）。
举例来说，原始的 GAN 论文中的生成器使用了 ReLU 和 sigmoid 激活函数，而判别器使用了 LeakyReLU 激活函数。但在实践中，可能会根据具体情况和实验结果选择不同的激活函数。

