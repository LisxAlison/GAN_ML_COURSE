1. 常见的激活函数选择：
ReLU（Rectified Linear Unit）： ReLU 是一个常用的激活函数，通常用于隐藏层。这个函数的输出是输入的正值（如果输入是负值，那么输出为0）。ReLU 由于其计算效率和有效性（比如帮助缓解梯度消失问题）在深度学习中广泛使用。
Leaky ReLU： Leaky ReLU 是 ReLU 的一个变体，它允许负输入有一个小的非零输出，这样可以防止神经元完全死亡，即永久性不活动。
Tanh： Tanh（双曲正切）函数将输入值压缩到 -1 和 1 之间，这在一些情况下是很有用的，例如当输出需要有一定范围时。
Sigmoid： Sigmoid 函数将输入值压缩到 0 和 1 之间。这在二元分类问题中很有用，例如在判别器的输出层，我们可能会使用 sigmoid 函数来表示生成的数据是真实的（接近 1）或假的（接近 0）。
举例来说，原始的 GAN 论文中的生成器使用了 ReLU 和 sigmoid 激活函数，而判别器使用了 LeakyReLU 激活函数。但在实践中，可能会根据具体情况和实验结果选择不同的激活函数。

2. 批量标准化（Batch Normalization）是一种在深度神经网络中常用的技术，用于改善网络训练过程的稳定性和效率。这个技术是由 Sergey Ioffe 和 Christian Szegedy 在 2015 年提出的。
训练深度神经网络时，由于每一层的输入都依赖于上一层的参数，这就导致了所谓的“内部协变量偏移”（internal covariate shift），即网络中每一层输入分布的变化。这种分布的变化可能会减慢网络的训练速度，因为每一层都需要不断地适应其输入数据的新分布。
批量标准化的目标就是减少这种内部协变量偏移。具体来说，批量标准化在每个训练批次中，对每一层的输入进行归一化，即使得输入的均值为 0，方差为 1。然后，批量标准化引入了两个可以学习的参数，一个是缩放因子（scale factor），一个是偏移因子（shift factor），用于对归一化后的数据进行缩放和移位，这样每一层可以学习到最合适的输入数据分布。
批量标准化有以下几个优点：
  1）更快的训练速度： 由于每一层的输入都具有更稳定的分布，网络可以使用更高的学习率进行训练，从而加速训练过程。
  2）更容易训练深层网络： 批量标准化可以帮助减少梯度消失问题，从而使得更深层的网络也能够有效地训练。
  3）降低对初始化的依赖： 批量标准化使得网络的训练过程对参数初始化的依赖度降低，因为每一层的输入分布都被标准化了。
  4）充当正则化： 批量标准化的一种副作用是它引入了一种微弱的正则化效果，因为每个批次的统计量会有些许的噪声，这就给模型的训练引入了一点噪声，有助于防止过拟合。
  5）允许使用一些激活函数： 批量标准化允许我们使用本来可能会导致训练问题的激活函数，例如 sigmoid 或者 tanh。

3. 卷积
1）在判别器中的卷积
在判别器中，输入通常是一张图像（无论是真实的还是生成器生成的）。卷积核（或称为滤波器）在图像上滑动，将自身与图像的每一个局部区域进行点乘，生成新的特征图（Feature Map）。这样，卷积操作可以提取输入图像的局部特征，并且具有平移不变性，即无论目标特征在图像的何处，都能被同样的卷积核提取出来。
2）在生成器中的卷积
在生成器中，通常使用的是转置卷积（Transposed Convolution）或称为反卷积（Deconvolution），这是一种特殊的卷积操作，它可以将较小的输入张量（tensor）变换为较大的输出张量。也就是说，它"放大"了输入数据。在这里，卷积操作是作用在输入噪声（通常是从某种分布中随机采样得到的）上的，经过一系列的转置卷积和非线性激活操作，生成器可以将这个噪声变换为一张新的图像。
因此，在判别器中，卷积用于提取图像的特征；而在生成器中，转置卷积则用于生成新的图像。

4. 池化（Pooling）：
池化是卷积神经网络中的一个步骤，其目的是降低特征图的空间大小（宽度和高度），以减少参数数量和计算量，防止过拟合，并保持特征的空间层次性。池化操作通常会在连续的卷积层之间进行。
最常见的池化操作是最大池化（Max Pooling）和平均池化（Average Pooling）。最大池化是选取窗口内的最大值作为输出，而平均池化则是计算窗口内所有值的平均值。这些操作都可以减少特征图的大小，同时保留重要的信息。

5. 上采样（Upsampling）：
上采样是一种增大特征图空间大小（宽度和高度）的操作，这在一些任务中是有用的，例如图像生成、语义分割等。在 GANs 中，特别是在生成器中，上采样通常用于从一个较小的、随机生成的向量开始，逐渐生成出一个完整的图像。
上采样通常通过转置卷积（也被称为反卷积）来实现。转置卷积在输入特征图的每个点周围添加零，然后进行正常的卷积操作。这样，输出的特征图就比输入的特征图大。通过这种方法，生成器可以逐渐"放大"其输出图像。

池化和上采样在网络中起到了相反的作用。池化用于降低特征图的空间尺寸，而上采样则用于增大特征图的空间尺寸。
在 GANs 中，判别器通常会使用池化操作，而生成器则会使用上采样操作。

6. 转置卷积（Transposed Convolutions）
在生成对抗网络（GAN）中，尤其是在生成器部分，常常会用到一种被称为转置卷积（Transposed Convolution）的操作，也有时被称为反卷积（Deconvolution），尽管后者的名称可能引起一些误解。
转置卷积的目标是将一个较小的数据张量（tensor）转换为一个较大的数据张量。具体来说，如果普通的卷积操作会将输入图像（或特征图）的大小从 WxH 降低到了 W'xH'，那么转置卷积则会将大小为 W'xH' 的输入扩大到 WxH。这就是为什么它在生成器中常常被使用，因为生成器的目标是从一个较小的随机噪声向量开始，逐步生成一个大的、高质量的图像。
转置卷积的过程大概是这样的：
首先，在输入张量的每个元素周围插入一些零。这使得输入张量的大小变大。
然后，将卷积核在这个大的张量上滑动，计算卷积。由于输入张量的大小已经变大了，所以卷积的结果也会比原来的输入张量更大。
通过一系列的转置卷积和非线性激活函数，可以逐步将一个较小的噪声向量变换为一张较大的图像。
需要注意的是，尽管转置卷积能够增大输入的大小，但这并不意味着它能够完全“逆转”普通的卷积操作。因此，将它称为“反卷积”可能会引起误解。
