1. 常见的激活函数选择：
ReLU（Rectified Linear Unit）： ReLU 是一个常用的激活函数，通常用于隐藏层。这个函数的输出是输入的正值（如果输入是负值，那么输出为0）。ReLU 由于其计算效率和有效性（比如帮助缓解梯度消失问题）在深度学习中广泛使用。
Leaky ReLU： Leaky ReLU 是 ReLU 的一个变体，它允许负输入有一个小的非零输出，这样可以防止神经元完全死亡，即永久性不活动。
Tanh： Tanh（双曲正切）函数将输入值压缩到 -1 和 1 之间，这在一些情况下是很有用的，例如当输出需要有一定范围时。
Sigmoid： Sigmoid 函数将输入值压缩到 0 和 1 之间。这在二元分类问题中很有用，例如在判别器的输出层，我们可能会使用 sigmoid 函数来表示生成的数据是真实的（接近 1）或假的（接近 0）。
举例来说，原始的 GAN 论文中的生成器使用了 ReLU 和 sigmoid 激活函数，而判别器使用了 LeakyReLU 激活函数。但在实践中，可能会根据具体情况和实验结果选择不同的激活函数。

2.批量标准化（Batch Normalization）是一种在深度神经网络中常用的技术，用于改善网络训练过程的稳定性和效率。这个技术是由 Sergey Ioffe 和 Christian Szegedy 在 2015 年提出的。
训练深度神经网络时，由于每一层的输入都依赖于上一层的参数，这就导致了所谓的“内部协变量偏移”（internal covariate shift），即网络中每一层输入分布的变化。这种分布的变化可能会减慢网络的训练速度，因为每一层都需要不断地适应其输入数据的新分布。
批量标准化的目标就是减少这种内部协变量偏移。具体来说，批量标准化在每个训练批次中，对每一层的输入进行归一化，即使得输入的均值为 0，方差为 1。然后，批量标准化引入了两个可以学习的参数，一个是缩放因子（scale factor），一个是偏移因子（shift factor），用于对归一化后的数据进行缩放和移位，这样每一层可以学习到最合适的输入数据分布。
批量标准化有以下几个优点：
1.更快的训练速度： 由于每一层的输入都具有更稳定的分布，网络可以使用更高的学习率进行训练，从而加速训练过程。
2.更容易训练深层网络： 批量标准化可以帮助减少梯度消失问题，从而使得更深层的网络也能够有效地训练。
3.降低对初始化的依赖： 批量标准化使得网络的训练过程对参数初始化的依赖度降低，因为每一层的输入分布都被标准化了。
4.充当正则化： 批量标准化的一种副作用是它引入了一种微弱的正则化效果，因为每个批次的统计量会有些许的噪声，这就给模型的训练引入了一点噪声，有助于防止过拟合。
5.允许使用一些激活函数： 批量标准化允许我们使用本来可能会导致训练问题的激活函数，例如 sigmoid 或者 tanh。
总的来说，批量标准化是一种强大的工具，可以帮助我们更有效地训练深度神经网络。
